{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # Html search tool\n",
    "import re # Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith requests.Session() as main_session:\\n   city_state_abbr = \\'raleigh-nc/\\' #*****change this city to what you want*****\\n   url = \\'https://www.zillow.com/\\' + city_state_abbr\\n   main_request = main_session.get(url, headers=request_headers)\\n\\n# After finding urls from the main page, use those links to open homedetaills page\\nwith requests.Session() as branch_session:\\n    branch_base_url = \"https://www.zillow.com/homedetails/\"\\n    branch_sub_url = \"5904-Endsley-Ct-Raleigh-NC-27610/65332868_zpid/\"\\n\\n    branch_request = branch_session.get(branch_base_url + branch_sub_url, headers=request_headers)\\n\\nbranch_soup = BeautifulSoup(branch_request.content, \"html.parser\")'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's not use context managers for now?\n",
    "\"\"\"\n",
    "with requests.Session() as main_session:\n",
    "   city_state_abbr = 'raleigh-nc/' #*****change this city to what you want*****\n",
    "   url = 'https://www.zillow.com/' + city_state_abbr\n",
    "   main_request = main_session.get(url, headers=request_headers)\n",
    "\n",
    "# After finding urls from the main page, use those links to open homedetaills page\n",
    "with requests.Session() as branch_session:\n",
    "    branch_base_url = \"https://www.zillow.com/homedetails/\"\n",
    "    branch_sub_url = \"5904-Endsley-Ct-Raleigh-NC-27610/65332868_zpid/\"\n",
    "\n",
    "    branch_request = branch_session.get(branch_base_url + branch_sub_url, headers=request_headers)\n",
    "\n",
    "branch_soup = BeautifulSoup(branch_request.content, \"html.parser\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#add contents of urls to soup variable from each url\\nmain_request_soup = BeautifulSoup(main_request.content, \"html.parser\")\\n\\n# From intial queried page (e.g., homes in Raleigh) find all links to homedetails page,\\n# which will have more detailed information on the home.\\n\\n# This will be the main base where the scraper will come back to after scraping each\\n# home details page. \\n\\n# Example: queried page -> homedetails page -> scrape data -> queried page \\n# Loop until all homes are scraped, then proceed to next page.\\nhomedetails_urls_list = main_request_soup.find_all(href=re.compile(\"homedetails\"))\\n# homedetails_urls_list = main_request_soup.select(\\'a[href^=\"https://www.zillow.com/homedetails/\"]\\')\\n\\nprint(len(homedetails_urls_list))\\nprint(main_request.links)'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#add contents of urls to soup variable from each url\n",
    "main_request_soup = BeautifulSoup(main_request.content, \"html.parser\")\n",
    "\n",
    "# From intial queried page (e.g., homes in Raleigh) find all links to homedetails page,\n",
    "# which will have more detailed information on the home.\n",
    "\n",
    "# This will be the main base where the scraper will come back to after scraping each\n",
    "# home details page. \n",
    "\n",
    "# Example: queried page -> homedetails page -> scrape data -> queried page \n",
    "# Loop until all homes are scraped, then proceed to next page.\n",
    "homedetails_urls_list = main_request_soup.find_all(href=re.compile(\"homedetails\"))\n",
    "# homedetails_urls_list = main_request_soup.select('a[href^=\"https://www.zillow.com/homedetails/\"]')\n",
    "\n",
    "print(len(homedetails_urls_list))\n",
    "print(main_request.links)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header required to circumvent captcha\n",
    "request_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "city_state_abbr = 'raleigh-nc/' #*****change this city to what you want*****\n",
    "url = 'https://www.zillow.com/' + city_state_abbr\n",
    "\n",
    "main_request = requests.get(url, headers=request_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2708-Boxelder-Ct-Raleigh-NC-27610/6485797', '2108-Ramseur-St-Raleigh-NC-27610/6398826', '3204-Plaza-Pl-Raleigh-NC-27612/6515233', '3528-Dewing-Dr-Raleigh-NC-27616/6563121', '11905-Straight-A-Way-Ln-Raleigh-NC-27613/6441712', '8631-Neuse-Club-Ln-APT-108-Raleigh-NC-27616/132294843', '2613-Elmhurst-Cir-Raleigh-NC-27610/6378853', '1621-Whittington-Dr-Raleigh-NC-27614/6545184', '8308-Grey-Abbey-Pl-Raleigh-NC-27615/6493189', '5412-Seafarer-Ct-Raleigh-NC-27613/6553583', '4813-Abundance-Ave-Raleigh-NC-27616/336869352', '4800-Patton-Ridge-Ct-Raleigh-NC-27612/60535350', '10316-20th-Rd-Raleigh-NC-27603/132205062', '8010-Upper-Lake-Dr-Raleigh-NC-27615/120875871', '12600-Old-Creedmoor-Rd-Raleigh-NC-27613/6466276', '1616-Wynne-Trace-Ct-Raleigh-NC-27603/119341893', '3008-Dawnbrook-Dr-Raleigh-NC-27604/53461261', '11301-Dunleith-Dr-Raleigh-NC-27614/6479003', '2618-Masonboro-Ct-2618-Raleigh-NC-27604/2084168050', '4300-Camelot-Dr-Raleigh-NC-27609/6435731', '7800-Foxwood-Dr-Raleigh-NC-27615/6472689', '125-Trails-End-Ct-Raleigh-NC-27614/6492897', '4306-Timberwood-Dr-Raleigh-NC-27612/6493671', '4601-Landover-Woods-Ln-Raleigh-NC-27616/65333576', '3208-Planet-Dr-Raleigh-NC-27604/50109171', '4105-Crowfield-Dr-Raleigh-NC-27610/53459623', '5920-Dunbarton-Way-Raleigh-NC-27613/6515529', '5901-Rivercliff-Ct-Raleigh-NC-27610/65332932', '1415-Beacon-Village-Dr-Raleigh-NC-27604/6558186', '2717-Hiking-Trl-Raleigh-NC-27615/6495378', '4221-Laurel-Ridge-Dr-Raleigh-NC-27612/6428993', '5904-Endsley-Ct-Raleigh-NC-27610/65332868', '1908-Jupiter-Hills-Ct-Raleigh-NC-27604/6530660', '4725-Royal-Troon-Dr-Raleigh-NC-27604/6524599', '8801-Wildwood-Links-Raleigh-NC-27613/6495515', '11308-Derby-Ln-Raleigh-NC-27613/6441812', '906-Brooks-Ave-Raleigh-NC-27607/6426706', '4216-Loon-Ln-Raleigh-NC-27616/6505225', '4209-Willow-Lake-Rd-Raleigh-NC-27616/6547491', '12816-Beech-Wood-Ct-Raleigh-NC-27614/6447463'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main_re_search = set(re.findall(\"https://www.zillow.com/homedetails/(.+?)_zpid\", str(main_request.content)))\n",
    "#re_search_set = set(main_re_search) # Set creates unique list - could change this in future if needed.\n",
    "print(main_re_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch_base_url = \"https://www.zillow.com/homedetails/\"\n",
    "# branch_sub_url = \"5904-Endsley-Ct-Raleigh-NC-27610/65332868_zpid/\"\n",
    "\n",
    "# branch_request = branch_session.get(branch_base_url + branch_sub_url, headers=request_headers)\n",
    "\n",
    "# branch_response = requests.get(branch_base_url + branch_sub_url, headers=request_headers)\n",
    "\n",
    "# branch_response_soup = BeautifulSoup(branch_response.content, \"html.parser\")\n",
    "# branch_response_soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_url_branch(re_search_set):\n",
    "\n",
    "    full_url_list = []\n",
    "\n",
    "    for url in re_search_set:\n",
    "        #print(url)\n",
    "        full_url = \"https://www.zillow.com/homedetails/\" + url + \"_zpid\"\n",
    "        #print(full_url)\n",
    "        full_url_list.append(full_url)\n",
    "    \n",
    "    return full_url_list\n",
    "\n",
    "print(get_full_url_branch(main_re_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43.3s\n",
    "def get_branch_request(full_url_list):\n",
    "\n",
    "    branch_response_good_soup_list = []\n",
    "\n",
    "    for url in full_url_list:\n",
    "        branch_response = requests.get(url, headers=request_headers)\n",
    "        branch_response_good_soup = BeautifulSoup(branch_response.content, \"html.parser\")\n",
    "        branch_response_good_soup_list.append(branch_response_good_soup)\n",
    "\n",
    "    return branch_response_good_soup_list\n",
    "\n",
    "#print(get_branch_request(get_full_url_branch(main_re_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = (get_branch_request(get_full_url_branch(main_re_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_var[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var_2 = scrape_homedetails(test_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that goes into the homedetails branch url and grabs\n",
    "# Price, beds, baths, sqft, days on zillow, views, and saves\n",
    "\n",
    "def scrape_homedetails(branch_homedetails_url_list):\n",
    "    \"\"\"\n",
    "    Scrape the homedetails url for price, beds, baths, \n",
    "    sqft, days on zillow, views, and saves\n",
    "    \"\"\"\n",
    "\n",
    "    page_scraped_data_list = []\n",
    "\n",
    "    for soup in branch_homedetails_url_list:\n",
    "        # Raw soups to be used below\n",
    "        raw_price_soup = soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\")\n",
    "        #raw_bed_soup = soup.find_all(\"strong\")\n",
    "        raw_address_soup = soup.find(\"h1\")\n",
    "\n",
    "        # Price\n",
    "        raw_price_soup = str(soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\"))\n",
    "        price_match_object = re.search(\"<span>(.+?)</span>\", raw_price_soup)\n",
    "        price = price_match_object.group(1)\n",
    "\n",
    "        # Attributes list: 0 Bed, 1 bath,  2 sqft, 3 days on zillow, 5 views, 6 saves\n",
    "        raw_bed_soup = str(soup.find_all(\"strong\"))\n",
    "        raw_bed_soup_list = raw_bed_soup.split(', ')\n",
    "\n",
    "        \n",
    "        count_beds = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[0]).group(1)\n",
    "        count_baths = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[1]).group(1)\n",
    "        sq_ft = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[2]).group(1)\n",
    "        days_on_zillow = re.search(\"<strong>(.+?)\", raw_bed_soup_list[3]).group(1)\n",
    "        count_views = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[4]).group(1)\n",
    "        count_saves = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[5]).group(1)\n",
    "\n",
    "        # Done\n",
    "        address = str(raw_address_soup)\n",
    "        address_1 = re.search(\"kHeRng(.+?)<!--\", address).group(1)[2:]\n",
    "        address_2 = re.search(\"<!-- -->(.+?)</h1>\", address).group(1)[9:]\n",
    "        address_full = address_1 + ' ' + address_2\n",
    "\n",
    "        # Aggregate into list\n",
    "        aggregate_list = [address_full, price, count_beds, count_baths, sq_ft, days_on_zillow,\n",
    "                        count_views, count_saves]\n",
    "\n",
    "        page_scraped_data_list.append(aggregate_list)\n",
    "    \n",
    "    return page_scraped_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<strong>4</strong>,\n",
      "<strong>3</strong>,\n",
      "<strong>1,900</strong>,\n",
      "<strong>8\n",
      "days</strong>,\n",
      "<strong>3,164</strong>,\n",
      "<strong>226</strong>,\n",
      "<strong>FRI</strong>,\n",
      "<strong>SAT</strong>,\n",
      "<strong>SUN</strong>,\n",
      "<strong>MON</strong>]\n",
      "11\n",
      "<class 'list'>\n",
      "3,164\n"
     ]
    }
   ],
   "source": [
    "omg = str(test_var[1].find_all(\"strong\"))\n",
    "#print(str(test_var[0].find_all(\"strong\")).split(' '))\n",
    "omg = omg.split(' ')\n",
    "\n",
    "for i in omg:\n",
    "    print(i)\n",
    "\n",
    "print(len(omg))\n",
    "print(type(omg))\n",
    "\n",
    "count_views = re.search(\"<strong>(.+?)</strong>\", omg[5]).group(1)\n",
    "print(count_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2108 Ramseur St, Raleigh, NC 27610', '$449,900', '4', '3', '1,900', '8', '3,164', '226']\n"
     ]
    }
   ],
   "source": [
    "test_scrape = scrape_homedetails(test_var)\n",
    "print(test_scrape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.zillow.com/homedetails/4601-Landover-Woods-Ln-Raleigh-NC-27616/65333576_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/5412-Seafarer-Ct-Raleigh-NC-27613/6553583_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/3008-Dawnbrook-Dr-Raleigh-NC-27604/53461261_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/12816-Beech-Wood-Ct-Raleigh-NC-27614/6447463_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/8631-Neuse-Club-Ln-APT-108-Raleigh-NC-27616/132294843_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/4216-Loon-Ln-Raleigh-NC-27616/6505225_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/7800-Foxwood-Dr-Raleigh-NC-27615/6472689_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/4306-Timberwood-Dr-Raleigh-NC-27612/6493671_zpid/', 'tabindex=-1><div', 'https://www.zillow.com/homedetails/11301-Dunleith-Dr-Raleigh-NC-27614/6479003_zpid/', 'tabindex=-1><div']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# I'd expect ~40 distinct urls\n",
    "def get_all_branch_urls():\n",
    "\n",
    "    url_list = []\n",
    "\n",
    "    for i in find_all_keyword_units():\n",
    "        url_list.append(strip_list_to_url(i))\n",
    "    \n",
    "    print(url_list)\n",
    "\n",
    "get_all_branch_urls()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(find_all_keyword_units()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsure of use of code below this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_output_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(test_output)\n\u001b[1;32m      2\u001b[0m href_url \u001b[39m=\u001b[39m test_output_str\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m5\u001b[39m]\n\u001b[1;32m      3\u001b[0m href_url_strip \u001b[39m=\u001b[39m href_url\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mhref=\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_output' is not defined"
     ]
    }
   ],
   "source": [
    "test_output_str = str(test_output)\n",
    "href_url = test_output_str.split(' ')[5]\n",
    "href_url_strip = href_url.replace(\"href=\",\"\")\n",
    "href_url_strip = href_url_strip.replace('\"','')\n",
    "print(href_url_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply testing function, can remove\n",
    "test_output = find_all_keyword_units(keyword = \"homedetails\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced with regular expression re?\n",
    "# Find all sections from main_soup that have homedetails. This will be used to get homedetails urls\n",
    "def find_all_keyword_units(keyword=\"homedetails\"):\n",
    "    \"\"\"Returns a list of keyword matches\"\"\"\n",
    "    keyword_match_list = main_request_soup.find_all(href=re.compile(str(keyword)))\n",
    "    return keyword_match_list\n",
    "\n",
    "# Replaced with re?\n",
    "# Create function to take list and return pruned list including only urls for homedetails\n",
    "def strip_list_to_url(html_list, index=5):\n",
    "    \"\"\"Takes keyword soup search, parses it into a list, and selects the homedetails url\"\"\"\n",
    "    list_to_str = str(html_list)\n",
    "    href_url = list_to_str.split(' ')[index] # Selects index of href with housedetails\n",
    "    href_url_strip = href_url.replace(\"href=\",\"\")\n",
    "    href_url_strip = href_url_strip.replace('\"','')\n",
    "    return href_url_strip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pagination \n",
    "\n",
    "only difference is 2_p for page 2 3_p for page 3\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%7D\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/2_p/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%2C%22pagination%22%3A%7B%22currentPage%22%3A2%7D%7D\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/3_p/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%2C%22pagination%22%3A%7B%22currentPage%22%3A3%7D%7D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# https://blog.devgenius.io/scraping-zillow-with-python-and-beautifulsoup-bbc7e581c218\\n\\n#create the first two dataframes\\ndf = pd.DataFrame()\\ndf1 = pd.DataFrame()\\n#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\\nfor i in soup:\\n    address = soup.find_all (class_= \\'list-card-addr\\')\\n    price = list(soup.find_all (class_=\\'list-card-price\\'))\\n    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\\n    details = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-details\\'})\\n    home_type = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-footer\\'})\\n    last_updated = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-top\\'})\\n    brokerage = list(soup.find_all(class_= \\'list-card-brokerage list-card-img-overlay\\',text=True))\\n    link = soup.find_all (class_= \\'list-card-link\\')\\n    \\n    #create dataframe columns out of variables\\n    df[\\'prices\\'] = price\\n    df[\\'address\\'] = address\\n    df[\\'beds\\'] = beds\\n\\n#create empty url list\\nurls = []\\n\\n#loop through url, pull the href and strip out the address tag\\nfor link in soup.find_all(\"article\"):\\n    href = link.find(\\'a\\',class_=\"list-card-link\")\\n    addresses = href.find(\\'address\\')\\n    addresses.extract()\\n    urls.append(href)\\n\\n#import urls into a links column\\ndf[\\'links\\'] = urls\\ndf[\\'links\\'] = df[\\'links\\'].astype(\\'str\\')\\n\\n#remove html tags\\ndf[\\'links\\'] = df[\\'links\\'].replace(\\'<a class=\"list-card-link\" href=\"\\', \\' \\', regex=True)\\ndf[\\'links\\'] = df[\\'links\\'].replace(\\'\" tabindex=\"0\"></a>\\', \\' \\', regex=True)\\n'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty sure I don't need this...\n",
    "\"\"\"\n",
    "# https://blog.devgenius.io/scraping-zillow-with-python-and-beautifulsoup-bbc7e581c218\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717 Hiking Trl,\n",
      "Raleigh, NC 27615\n",
      "[<span class=\"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\" data-testid=\"price\"><span>$352,000</span></span>]\n",
      "$352,000\n",
      "beds 3\n",
      "baths 3\n",
      "sq_ft 1,566\n",
      "days_on_zillow 8\n",
      "count_views 3,931\n",
      "count_saves 229\n",
      "[<span class=\"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\" data-testid=\"price\"><span>$352,000</span></span>]\n",
      "[<strong>3</strong>, <strong>3</strong>, <strong>1,566</strong>, <strong>8 days</strong>, <strong>3,940</strong>, <strong>229</strong>, <strong>WED</strong>, <strong>THU</strong>, <strong>FRI</strong>, <strong>SAT</strong>]\n",
      "<h1 class=\"Text-c11n-8-73-0__sc-aiai24-0 kHeRng\">5904 Endsley Ct,<!-- -->Â <!-- -->Raleigh, NC 27610</h1>\n"
     ]
    }
   ],
   "source": [
    "# Appendix:\n",
    "\n",
    "# Done - address\n",
    "address = str(raw_address_soup)\n",
    "address_1 = re.search(\"kHeRng(.+?)<!--\", address).group(1)[2:]\n",
    "address_2 = re.search(\"<!-- -->(.+?)</h1>\", address).group(1)[9:]\n",
    "\n",
    "\n",
    "print(address_1)\n",
    "print(address_2)\n",
    "\n",
    "# Price\n",
    "raw_price_soup = str(branch_soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\"))\n",
    "price_match_object = re.search(\"<span>(.+?)</span>\", raw_price_soup)\n",
    "price = price_match_object.group(1)\n",
    "\n",
    "print(raw_price_soup)\n",
    "print(price)\n",
    "\n",
    "# Done\n",
    "# Get all attributes besides price and address\n",
    "# Turn this into a function bb\n",
    "\n",
    "# 0 Bed, 1 bath,  2 sqft, 3 days on zillow, 5 views, 6 saves\n",
    "raw_bed_soup = str(branch_soup.find_all(\"strong\"))\n",
    "raw_bed_soup_list = raw_bed_soup.split(' ')\n",
    "count_beds = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[0]).group(1)\n",
    "count_baths = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[1]).group(1)\n",
    "sq_ft = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[2]).group(1)\n",
    "days_on_zillow = re.search(\"<strong>(.+?)\", raw_bed_soup_list[3]).group(1)\n",
    "count_views = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[5]).group(1)\n",
    "count_saves = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[6]).group(1)\n",
    "\n",
    "\n",
    "print(\"beds\",count_beds)\n",
    "print(\"baths\", count_baths)\n",
    "print(\"sq_ft\", sq_ft)\n",
    "print(\"days_on_zillow\", days_on_zillow)\n",
    "print(\"count_views\", count_views)\n",
    "print(\"count_saves\", count_saves)\n",
    "\n",
    "#raw soups\n",
    "raw_price_soup = branch_response_soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\")\n",
    "raw_bed_soup = branch_response_soup.find_all(\"strong\")\n",
    "raw_address_soup = branch_response_soup.find(\"h1\")\n",
    "\n",
    "print(raw_price_soup) # done\n",
    "print(raw_bed_soup) # done\n",
    "print(raw_address_soup) # done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('really-estate-mG1FNr6d-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0eff05228884e12a81e7c0d2b79ca80cfc67c833393b3fe7ac3844788baf07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
