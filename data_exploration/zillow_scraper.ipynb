{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # Html search tool\n",
    "import re # Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith requests.Session() as main_session:\\n   city_state_abbr = \\'raleigh-nc/\\' #*****change this city to what you want*****\\n   url = \\'https://www.zillow.com/\\' + city_state_abbr\\n   main_request = main_session.get(url, headers=request_headers)\\n\\n# After finding urls from the main page, use those links to open homedetaills page\\nwith requests.Session() as branch_session:\\n    branch_base_url = \"https://www.zillow.com/homedetails/\"\\n    branch_sub_url = \"5904-Endsley-Ct-Raleigh-NC-27610/65332868_zpid/\"\\n\\n    branch_request = branch_session.get(branch_base_url + branch_sub_url, headers=request_headers)\\n\\nbranch_soup = BeautifulSoup(branch_request.content, \"html.parser\")'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's not use context managers for now?\n",
    "\"\"\"\n",
    "with requests.Session() as main_session:\n",
    "   city_state_abbr = 'raleigh-nc/' #*****change this city to what you want*****\n",
    "   url = 'https://www.zillow.com/' + city_state_abbr\n",
    "   main_request = main_session.get(url, headers=request_headers)\n",
    "\n",
    "# After finding urls from the main page, use those links to open homedetaills page\n",
    "with requests.Session() as branch_session:\n",
    "    branch_base_url = \"https://www.zillow.com/homedetails/\"\n",
    "    branch_sub_url = \"5904-Endsley-Ct-Raleigh-NC-27610/65332868_zpid/\"\n",
    "\n",
    "    branch_request = branch_session.get(branch_base_url + branch_sub_url, headers=request_headers)\n",
    "\n",
    "branch_soup = BeautifulSoup(branch_request.content, \"html.parser\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#add contents of urls to soup variable from each url\\nmain_request_soup = BeautifulSoup(main_request.content, \"html.parser\")\\n\\n# From intial queried page (e.g., homes in Raleigh) find all links to homedetails page,\\n# which will have more detailed information on the home.\\n\\n# This will be the main base where the scraper will come back to after scraping each\\n# home details page. \\n\\n# Example: queried page -> homedetails page -> scrape data -> queried page \\n# Loop until all homes are scraped, then proceed to next page.\\nhomedetails_urls_list = main_request_soup.find_all(href=re.compile(\"homedetails\"))\\n# homedetails_urls_list = main_request_soup.select(\\'a[href^=\"https://www.zillow.com/homedetails/\"]\\')\\n\\nprint(len(homedetails_urls_list))\\nprint(main_request.links)'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#add contents of urls to soup variable from each url\n",
    "main_request_soup = BeautifulSoup(main_request.content, \"html.parser\")\n",
    "\n",
    "# From intial queried page (e.g., homes in Raleigh) find all links to homedetails page,\n",
    "# which will have more detailed information on the home.\n",
    "\n",
    "# This will be the main base where the scraper will come back to after scraping each\n",
    "# home details page. \n",
    "\n",
    "# Example: queried page -> homedetails page -> scrape data -> queried page \n",
    "# Loop until all homes are scraped, then proceed to next page.\n",
    "homedetails_urls_list = main_request_soup.find_all(href=re.compile(\"homedetails\"))\n",
    "# homedetails_urls_list = main_request_soup.select('a[href^=\"https://www.zillow.com/homedetails/\"]')\n",
    "\n",
    "print(len(homedetails_urls_list))\n",
    "print(main_request.links)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Header required to circumvent captcha\n",
    "request_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "city_state_abbr = \"raleigh-nc/\" #*****change this city to what you want*****\n",
    "url = \"https://www.zillow.com/\" + city_state_abbr + page_number + \"_p\"\n",
    "\n",
    "main_request = requests.get(url, headers=request_headers)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(city_state_abbr=\"raleigh-nc/\"):\n",
    "\n",
    "    url_setup = \"https://www.zillow.com/\" + city_state_abbr\n",
    "    main_request_setup = requests.get(url_setup, headers=request_headers)\n",
    "    page_soup = BeautifulSoup(main_request_setup.content).find_all(\"span\", class_ = \"Text-c11n-8-73-8__sc-aiai24-0 bKahKV\")\n",
    "\n",
    "    page_soup_max_number = int(re.search(\"Page <!-- -->1<!-- --> of <!-- -->(.+?)</span>\", str(page_soup)).group(1))\n",
    "\n",
    "    full_list_urls = []\n",
    "\n",
    "    for page_number in range(1, page_soup_max_number + 1):\n",
    "        url = \"https://www.zillow.com/\" + city_state_abbr + str(page_number) + \"_p\"\n",
    "        main_request = requests.get(url, headers=request_headers)\n",
    "        main_re_search = list(set(re.findall(\"https://www.zillow.com/homedetails/(.+?)_zpid\", str(main_request.content))))\n",
    "        full_list_urls.append(main_re_search)\n",
    "\n",
    "    flat_full_llist_urls = [item for sublist in full_list_urls for item in sublist]\n",
    "\n",
    "    return flat_full_llist_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buncha_urls = get_all_urls()\n",
    "len(buncha_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#main_re_search = set(re.findall(\"https://www.zillow.com/homedetails/(.+?)_zpid\", str(main_request.content)))\n",
    "#re_search_set = set(main_re_search) # Set creates unique list - could change this in future if needed.\n",
    "#print(main_re_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_url_branch(re_search_set):\n",
    "\n",
    "    full_url_list = []\n",
    "\n",
    "    for url in re_search_set:\n",
    "        #print(url)\n",
    "        full_url = \"https://www.zillow.com/homedetails/\" + url + \"_zpid\"\n",
    "        #print(full_url)\n",
    "        full_url_list.append(full_url)\n",
    "    \n",
    "    return full_url_list\n",
    "\n",
    "# print(get_full_url_branch(main_re_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43.3s\n",
    "def get_branch_request(full_url_list):\n",
    "\n",
    "    branch_response_good_soup_list = []\n",
    "\n",
    "    for url in full_url_list:\n",
    "        branch_response = requests.get(url, headers=request_headers)\n",
    "        branch_response_good_soup = BeautifulSoup(branch_response.content, \"html.parser\")\n",
    "        branch_response_good_soup_list.append(branch_response_good_soup)\n",
    "\n",
    "    return branch_response_good_soup_list\n",
    "\n",
    "#print(get_branch_request(get_full_url_branch(main_re_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that goes into the homedetails branch url and grabs\n",
    "# Price, beds, baths, sqft, days on zillow, views, and saves\n",
    "\n",
    "def scrape_homedetails(branch_homedetails_url_list):\n",
    "    \"\"\"\n",
    "    Scrape the homedetails url for price, beds, baths, \n",
    "    sqft, days on zillow, views, and saves\n",
    "    \"\"\"\n",
    "\n",
    "    page_scraped_data_list = []\n",
    "\n",
    "    for soup in branch_homedetails_url_list:\n",
    "        # Raw soups to be used below\n",
    "        raw_price_soup = soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\")\n",
    "        #raw_bed_soup = soup.find_all(\"strong\")\n",
    "        raw_address_soup = soup.find(\"h1\")\n",
    "\n",
    "        # Price\n",
    "        raw_price_soup = str(soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\"))\n",
    "        price_match_object = re.search(\"<span>(.+?)</span>\", raw_price_soup)\n",
    "        price = price_match_object.group(1)\n",
    "\n",
    "        # Attributes list: 0 Bed, 1 bath,  2 sqft, 3 days on zillow, 5 views, 6 saves\n",
    "        raw_bed_soup = str(soup.find_all(\"strong\"))\n",
    "        raw_bed_soup_list = raw_bed_soup.split(', ') # Holy balls spent 4 hours on this.\n",
    "\n",
    "        \n",
    "        count_beds = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[0]).group(1)\n",
    "        count_baths = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[1]).group(1)\n",
    "        sq_ft = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[2]).group(1)\n",
    "        days_on_zillow = re.search(\"<strong>(.+?)\", raw_bed_soup_list[3]).group(1)\n",
    "        count_views = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[4]).group(1)\n",
    "        count_saves = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[5]).group(1)\n",
    "\n",
    "        # Done\n",
    "        address = str(raw_address_soup)\n",
    "        address_1 = re.search(\"kHeRng(.+?)<!--\", address).group(1)[2:]\n",
    "        address_2 = re.search(\"<!-- -->(.+?)</h1>\", address).group(1)[9:]\n",
    "        address_full = address_1 + ' ' + address_2\n",
    "\n",
    "        # Aggregate into list\n",
    "        aggregate_list = [address_full, price, count_beds, count_baths, sq_ft, days_on_zillow,\n",
    "                        count_views, count_saves]\n",
    "\n",
    "        page_scraped_data_list.append(aggregate_list)\n",
    "    \n",
    "    return page_scraped_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [343], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m page_data_1 \u001b[39m=\u001b[39m scrape_homedetails(get_branch_request(get_full_url_branch(get_all_urls())))\n\u001b[1;32m      2\u001b[0m omg_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(page_data_1)\n\u001b[1;32m      3\u001b[0m omg_df\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn [300], line 32\u001b[0m, in \u001b[0;36mscrape_homedetails\u001b[0;34m(branch_homedetails_url_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m sq_ft \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m<strong>(.+?)</strong>\u001b[39m\u001b[39m\"\u001b[39m, raw_bed_soup_list[\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m days_on_zillow \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m<strong>(.+?)\u001b[39m\u001b[39m\"\u001b[39m, raw_bed_soup_list[\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m count_views \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m<strong>(.+?)</strong>\u001b[39m\u001b[39m\"\u001b[39m, raw_bed_soup_list[\u001b[39m4\u001b[39;49m])\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m count_saves \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m<strong>(.+?)</strong>\u001b[39m\u001b[39m\"\u001b[39m, raw_bed_soup_list[\u001b[39m5\u001b[39m])\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Done\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "page_data_1 = scrape_homedetails(get_branch_request(get_full_url_branch(get_all_urls())))\n",
    "omg_df = pd.DataFrame(page_data_1)\n",
    "omg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 8)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omg_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pagination \n",
    "\n",
    "only difference is 2_p for page 2 3_p for page 3\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%7D\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/2_p/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%2C%22pagination%22%3A%7B%22currentPage%22%3A2%7D%7D\n",
    "\n",
    "https://www.zillow.com/raleigh-nc/luxury-homes/3_p/?searchQueryState=%7B%22usersSearchTerm%22%3A%22Raleigh%2C%20NC%22%2C%22mapBounds%22%3A%7B%22west%22%3A-78.92658305517578%2C%22east%22%3A-78.32439494482422%2C%22south%22%3A35.45233694912461%2C%22north%22%3A36.119629123743984%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54047%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22priced%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%2C%22pagination%22%3A%7B%22currentPage%22%3A3%7D%7D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsure of use of code below this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [301], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_output_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(test_output)\n\u001b[1;32m      2\u001b[0m href_url \u001b[39m=\u001b[39m test_output_str\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m5\u001b[39m]\n\u001b[1;32m      3\u001b[0m href_url_strip \u001b[39m=\u001b[39m href_url\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mhref=\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_output' is not defined"
     ]
    }
   ],
   "source": [
    "test_output_str = str(test_output)\n",
    "href_url = test_output_str.split(' ')[5]\n",
    "href_url_strip = href_url.replace(\"href=\",\"\")\n",
    "href_url_strip = href_url_strip.replace('\"','')\n",
    "print(href_url_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply testing function, can remove\n",
    "test_output = find_all_keyword_units(keyword = \"homedetails\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced with regular expression re?\n",
    "# Find all sections from main_soup that have homedetails. This will be used to get homedetails urls\n",
    "def find_all_keyword_units(keyword=\"homedetails\"):\n",
    "    \"\"\"Returns a list of keyword matches\"\"\"\n",
    "    keyword_match_list = main_request_soup.find_all(href=re.compile(str(keyword)))\n",
    "    return keyword_match_list\n",
    "\n",
    "# Replaced with re?\n",
    "# Create function to take list and return pruned list including only urls for homedetails\n",
    "def strip_list_to_url(html_list, index=5):\n",
    "    \"\"\"Takes keyword soup search, parses it into a list, and selects the homedetails url\"\"\"\n",
    "    list_to_str = str(html_list)\n",
    "    href_url = list_to_str.split(' ')[index] # Selects index of href with housedetails\n",
    "    href_url_strip = href_url.replace(\"href=\",\"\")\n",
    "    href_url_strip = href_url_strip.replace('\"','')\n",
    "    return href_url_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# https://blog.devgenius.io/scraping-zillow-with-python-and-beautifulsoup-bbc7e581c218\\n\\n#create the first two dataframes\\ndf = pd.DataFrame()\\ndf1 = pd.DataFrame()\\n#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\\nfor i in soup:\\n    address = soup.find_all (class_= \\'list-card-addr\\')\\n    price = list(soup.find_all (class_=\\'list-card-price\\'))\\n    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\\n    details = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-details\\'})\\n    home_type = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-footer\\'})\\n    last_updated = soup.find_all (\\'div\\', {\\'class\\': \\'list-card-top\\'})\\n    brokerage = list(soup.find_all(class_= \\'list-card-brokerage list-card-img-overlay\\',text=True))\\n    link = soup.find_all (class_= \\'list-card-link\\')\\n    \\n    #create dataframe columns out of variables\\n    df[\\'prices\\'] = price\\n    df[\\'address\\'] = address\\n    df[\\'beds\\'] = beds\\n\\n#create empty url list\\nurls = []\\n\\n#loop through url, pull the href and strip out the address tag\\nfor link in soup.find_all(\"article\"):\\n    href = link.find(\\'a\\',class_=\"list-card-link\")\\n    addresses = href.find(\\'address\\')\\n    addresses.extract()\\n    urls.append(href)\\n\\n#import urls into a links column\\ndf[\\'links\\'] = urls\\ndf[\\'links\\'] = df[\\'links\\'].astype(\\'str\\')\\n\\n#remove html tags\\ndf[\\'links\\'] = df[\\'links\\'].replace(\\'<a class=\"list-card-link\" href=\"\\', \\' \\', regex=True)\\ndf[\\'links\\'] = df[\\'links\\'].replace(\\'\" tabindex=\"0\"></a>\\', \\' \\', regex=True)\\n'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty sure I don't need this...\n",
    "\"\"\"\n",
    "# https://blog.devgenius.io/scraping-zillow-with-python-and-beautifulsoup-bbc7e581c218\n",
    "\n",
    "#create the first two dataframes\n",
    "df = pd.DataFrame()\n",
    "df1 = pd.DataFrame()\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    #create dataframe columns out of variables\n",
    "    df['prices'] = price\n",
    "    df['address'] = address\n",
    "    df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    addresses = href.find('address')\n",
    "    addresses.extract()\n",
    "    urls.append(href)\n",
    "\n",
    "#import urls into a links column\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717 Hiking Trl,\n",
      "Raleigh, NC 27615\n",
      "[<span class=\"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\" data-testid=\"price\"><span>$352,000</span></span>]\n",
      "$352,000\n",
      "beds 3\n",
      "baths 3\n",
      "sq_ft 1,566\n",
      "days_on_zillow 8\n",
      "count_views 3,931\n",
      "count_saves 229\n",
      "[<span class=\"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\" data-testid=\"price\"><span>$352,000</span></span>]\n",
      "[<strong>3</strong>, <strong>3</strong>, <strong>1,566</strong>, <strong>8 days</strong>, <strong>3,940</strong>, <strong>229</strong>, <strong>WED</strong>, <strong>THU</strong>, <strong>FRI</strong>, <strong>SAT</strong>]\n",
      "<h1 class=\"Text-c11n-8-73-0__sc-aiai24-0 kHeRng\">5904 Endsley Ct,<!-- -->Â <!-- -->Raleigh, NC 27610</h1>\n"
     ]
    }
   ],
   "source": [
    "# Appendix:\n",
    "\n",
    "# Done - address\n",
    "address = str(raw_address_soup)\n",
    "address_1 = re.search(\"kHeRng(.+?)<!--\", address).group(1)[2:]\n",
    "address_2 = re.search(\"<!-- -->(.+?)</h1>\", address).group(1)[9:]\n",
    "\n",
    "\n",
    "print(address_1)\n",
    "print(address_2)\n",
    "\n",
    "# Price\n",
    "raw_price_soup = str(branch_soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\"))\n",
    "price_match_object = re.search(\"<span>(.+?)</span>\", raw_price_soup)\n",
    "price = price_match_object.group(1)\n",
    "\n",
    "print(raw_price_soup)\n",
    "print(price)\n",
    "\n",
    "# Done\n",
    "# Get all attributes besides price and address\n",
    "# Turn this into a function bb\n",
    "\n",
    "# 0 Bed, 1 bath,  2 sqft, 3 days on zillow, 5 views, 6 saves\n",
    "raw_bed_soup = str(branch_soup.find_all(\"strong\"))\n",
    "raw_bed_soup_list = raw_bed_soup.split(' ')\n",
    "count_beds = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[0]).group(1)\n",
    "count_baths = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[1]).group(1)\n",
    "sq_ft = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[2]).group(1)\n",
    "days_on_zillow = re.search(\"<strong>(.+?)\", raw_bed_soup_list[3]).group(1)\n",
    "count_views = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[5]).group(1)\n",
    "count_saves = re.search(\"<strong>(.+?)</strong>\", raw_bed_soup_list[6]).group(1)\n",
    "\n",
    "\n",
    "print(\"beds\",count_beds)\n",
    "print(\"baths\", count_baths)\n",
    "print(\"sq_ft\", sq_ft)\n",
    "print(\"days_on_zillow\", days_on_zillow)\n",
    "print(\"count_views\", count_views)\n",
    "print(\"count_saves\", count_saves)\n",
    "\n",
    "#raw soups\n",
    "raw_price_soup = branch_response_soup.find_all(\"span\", class_ = \"Text-c11n-8-73-0__sc-aiai24-0 dpf__sc-1me8eh6-0 kGdfMs fzJCbY\")\n",
    "raw_bed_soup = branch_response_soup.find_all(\"strong\")\n",
    "raw_address_soup = branch_response_soup.find(\"h1\")\n",
    "\n",
    "print(raw_price_soup) # done\n",
    "print(raw_bed_soup) # done\n",
    "print(raw_address_soup) # done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('really-estate-mG1FNr6d-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0eff05228884e12a81e7c0d2b79ca80cfc67c833393b3fe7ac3844788baf07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
